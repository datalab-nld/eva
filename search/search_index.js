var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Overview","text":"EVA METOC <p> Environmental Video Array - Making METOC data portable with video codecs </p> <p> Explore the docs \u00bb - Read the paper \u00bb </p>"},{"location":"index.html#about-the-project","title":"About The Project","text":"<p>EVA METOC is a package to compress Numerical Weather Predictions and Ocean forecast into small files, using video codecs.</p> <p>Numerical Weather Predictions and Ocean forecasts (METOC) are datasets with a large file size. Some users have limited bandwidth available and require much smaller file size that can be shared by email or satellite communications. Compression of the datasets can be a solution for these use cases. In this package, we provide tools to use common video codecs to make METOC data portable. Such codes are maintained for a very large user community and offer decompression in near-real time of small compressed datasets while preserving most of the important information.</p>"},{"location":"index.html#build-with","title":"Build with","text":""},{"location":"index.html#getting-started","title":"Getting started","text":""},{"location":"index.html#installation","title":"Installation","text":""},{"location":"index.html#ffmpeg","title":"FFmpeg","text":"<p>Since <code>evametoc</code> depends on ffmpeg being available, firstly install ffmpeg. </p> <p><pre><code>$ sudo apt update\n$ sudo apt install ffmpeg\n</code></pre> (For other platforms, please check the ffmpeg page. Make sure the <code>ffmpeg</code> command is available in you PATH.)</p>"},{"location":"index.html#clone-the-repo","title":"Clone the repo","text":"<pre><code>$ git clone https://github.com/datalab-nld/eva.git\n</code></pre>"},{"location":"index.html#prerequisites","title":"Prerequisites","text":"<p>Then install the required packages using <code>conda</code>, <code>mamba</code> or <code>pip</code></p> <pre><code>$ mamba create -n evametoc\n$ mamba activate evametoc\n$ mamba install --file requirements.txt\n</code></pre>"},{"location":"index.html#installation_1","title":"Installation","text":"<p>Lastly install this package <pre><code>$ python setup.py install \n</code></pre></p>"},{"location":"index.html#usage","title":"Usage","text":"<p>To encode a netcdf file with <code>evametoc</code>, use the <code>main_encode</code> function.</p> <pre><code>import evametoc\nimport os\n\nfilename = \"./HARM43_V1_P1.nc4\"\ncurrent_filesize = os.stat(filename).st_size\n\nevametoc.main_encode(\n    dataset_path = filename\n    file_size = int(current_filesize // 100),\n    target_path = \"HARM43_V1_P1.eva\"\n    dataset_type = 'harm40',\n    nan_lossless = True\n)\n</code></pre> <p>After transfering the <code>.eva</code>-file to the target platform, you can decompress the data with the <code>main_decode</code> function.</p> <pre><code>import evametoc\n\nevametoc.main_decode(\n    eva_path = \"./HARM43_V1_P1.eva\",\n    target_path = \"HARM43_V1_P1_transfer.nc4\"\n)\n</code></pre> <p>For more examples, please refer to the Documentation</p>"},{"location":"index.html#license","title":"License","text":"<p>Distributed under the EUROPEAN UNION PUBLIC LICENCE v. 1.2. See <code>LICENSE</code> for more information.</p>"},{"location":"reference/evametoc/core.html","title":"core","text":"<p>This module provides the main routines for encoding and decoding videos to environmental arrays.</p>"},{"location":"reference/evametoc/core.html#evametoc.core.main_decode","title":"<code>main_decode(eva_path, target_path=None, overwrite=False, **kwargs)</code>","text":"<p>Decodes a dataset from a .eva video-fileset</p> <p>Parameters:</p> <ul> <li> <code>eva_path</code>               (<code>str</code>)           \u2013            <p>Path to an environmental video array (.eva file)</p> </li> <li> <code>target_path</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Destination path to store the dataset/.nc-file</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Overwrite an existing file, if already present</p> </li> <li> <code>kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments passed along to video_decoder_ffmpeg</p> </li> </ul> Source code in <code>package/src/evametoc/core.py</code> <pre><code>def main_decode(eva_path, target_path=None, overwrite=False, **kwargs):\n    \"\"\"Decodes a dataset from a .eva video-fileset\n\n    Args:\n        eva_path (str): Path to an environmental video array (.eva file)\n        target_path (str|None): Destination path to store the dataset/.nc-file\n        overwrite (bool): Overwrite an existing file, if already present\n        kwargs (dict): Keyword arguments passed along to video_decoder_ffmpeg\n    \"\"\"\n    # Check function arguments\n    assert os.path.isfile(eva_path), f\"File '{eva_path}' does not exist\"\n\n    # Configure function arguments\n    target_path = evametoc.utils.find_target_path(eva_path, '.eva_transfer.nc', target_path, overwrite=overwrite)\n    if not overwrite:\n        assert not os.path.isfile(target_path), f\"File '{target_path}' does already exist\"\n\n    with tempfile.TemporaryDirectory(prefix='.evax_', dir='.') as tmp_dir_name:\n        evametoc.evafile.eva_to_dir(eva_path, tmp_dir_name)\n\n        arrays = {}\n        for video_file in glob.iglob(os.path.join(tmp_dir_name, '*.*')):\n            if video_file.endswith('.json'):\n                continue\n            metadata, metaspec = evametoc.metadata.read(video_file)\n            video_frames, metadata, metaspec = evametoc.video.ffmpeg.decode(video_file, metadata, metaspec, **kwargs)\n\n            xarrays = evametoc.video.frames.to_xarray(video_frames, metadata, metaspec)\n\n            for i in range(len(xarrays)):\n                if '__dummy' in xarrays[i].name:\n                    continue\n                arrays[xarrays[i].name] = xarrays[i]\n\n        dataset = xr.Dataset(arrays)\n        dataset.attrs['EVA'] = (\n            'Decompressed data from an {} environmental video array'.format(metaspec.get('_eva_compression_', '?B')))\n        dataset.attrs.update(metaspec['_dataset_attrs_'])\n\n        dataset = evametoc.data.postprocess.readd_no_data_params(dataset, metaspec)\n        dataset = evametoc.data.postprocess.merge_levels(dataset)\n\n        comp = {'zlib': True, 'complevel': 5, 'least_significant_digit': 3}\n        encoding = {var: comp for var in dataset.data_vars}\n        dataset.to_netcdf(target_path, format='NETCDF4', encoding=encoding)\n</code></pre>"},{"location":"reference/evametoc/core.html#evametoc.core.main_encode","title":"<code>main_encode(dataset_path, file_size, target_path=None, dataset_type=None, nan_lossless=True, **kwargs)</code>","text":"<p>Encodes a dataset to a .eva video-fileset</p> <p>Parameters:</p> <ul> <li> <code>dataset_path</code>               (<code>str</code>)           \u2013            <p>Path to a NetCDF dataset</p> </li> <li> <code>file_size</code>               (<code>int</code>)           \u2013            <p>Number of bytes to be used in the final file size (approx)</p> </li> <li> <code>target_path</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Destination path to the eva file</p> </li> <li> <code>dataset_type</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>One of ['harm40','GFS','copernicus_ocean',None] denoting the type of dataset</p> </li> <li> <code>nan_lossless</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Encode the NaNs losslessly in the metadata</p> </li> <li> <code>kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>to be passed along to video_encoder_ffmpeg</p> </li> </ul> Source code in <code>package/src/evametoc/core.py</code> <pre><code>def main_encode(dataset_path, file_size, target_path=None, dataset_type=None, nan_lossless=True, **kwargs):\n    \"\"\"Encodes a dataset to a .eva video-fileset\n\n    Args:\n        dataset_path (str): Path to a NetCDF dataset\n        file_size (int): Number of bytes to be used in the final file size (approx)\n        target_path (str|None): Destination path to the eva file\n        dataset_type (str): One of ['harm40','GFS','copernicus_ocean',None] denoting the type of dataset\n        nan_lossless (bool): Encode the NaNs losslessly in the metadata\n        kwargs (dict): to be passed along to video_encoder_ffmpeg\n    \"\"\"\n    # Check function arguments\n    assert os.path.isfile(dataset_path), f\"File '{dataset_path}' does not exist\"\n    assert dataset_type in ['harm40', 'GFS', 'copernicus_ocean',\n                            None], f\"Dataset-type '{dataset_type}' is not supported\"\n\n    # Configure function arguments\n    target_path = evametoc.utils.find_target_path(dataset_path, '.eva', target_path, overwrite=True)\n    video_file_ext = kwargs.pop('video_file_ext', 'mp4')\n\n    # Init\n    global_metadata = {}\n\n    # Main process\n    ds = xr.open_dataset(dataset_path)\n    global_metadata['_dataset_attrs_'] = ds.attrs.copy()\n    global_metadata['_eva_compression_'] = evametoc.utils.human_readable_file_size(file_size)\n\n    expected_nan_size_lossless = np.mean(np.array([ds[k][0, ...].size for k in ds.keys()]))/8/1.5\n\n    ds = evametoc.data.prepare.prepare(ds, dataset_type)\n    ds = evametoc.data.preprocess.split_levels(ds)\n    ds, global_metadata = evametoc.data.preprocess.remove_no_data_params(ds, global_metadata)\n    ds = evametoc.data.preprocess.make_num_params_multi3(ds)\n\n    groups = evametoc.data.group.by_unit(ds)\n    if file_size is None or file_size &lt;= 0:\n        file_size_per_video = None\n    else:\n        file_size_per_video = file_size / len(groups)\n\n    with tempfile.TemporaryDirectory(prefix='.evac_', dir='.') as tmp_dir_name:\n        for group_id, group in enumerate(groups):\n            frames, metadata = evametoc.video.frames.from_xarrays(\n                ds[group[0]],\n                ds[group[1]],\n                ds[group[2]],\n                shared_coords=True,\n                nan_lossless=nan_lossless)\n\n            video_file_path = os.path.join(tmp_dir_name, f'video_{group_id:04d}.{video_file_ext:s}')\n            if group_id == 0:\n                metadata.update(global_metadata)\n            video_file_path, metadata = evametoc.video.ffmpeg.encode(\n                video_file=video_file_path,\n                video_frames=frames,\n                metadata=metadata,\n                file_size=file_size_per_video,\n                **kwargs)\n            evametoc.metadata.write(video_file_path, metadata)\n        evametoc.evafile.dir_to_eva(tmp_dir_name, target_path)\n</code></pre>"},{"location":"reference/evametoc/evafile.html","title":"evafile","text":"<p>This module provides routines compress and decompress Environmental Video Array (.eva)-files</p>"},{"location":"reference/evametoc/evafile.html#evametoc.evafile.dir_to_eva","title":"<code>dir_to_eva(input_dir, output_eva_file)</code>","text":"<p>Compresses a dir into a zip renamed to a .eva file</p> <p>Parameters:</p> <ul> <li> <code>input_dir</code>               (<code>str</code>)           \u2013            <p>Path to a directory</p> </li> <li> <code>output_eva_file</code>               (<code>str</code>)           \u2013            <p>output file name</p> </li> </ul> Source code in <code>package/src/evametoc/evafile.py</code> <pre><code>def dir_to_eva(input_dir, output_eva_file):\n    \"\"\"Compresses a dir into a zip renamed to a .eva file\n\n    Args:\n        input_dir (str): Path to a directory\n        output_eva_file (str): output file name\n    \"\"\"\n    shutil.make_archive(output_eva_file, 'zip', input_dir)\n    os.rename(str(output_eva_file) + \".zip\", str(output_eva_file))\n</code></pre>"},{"location":"reference/evametoc/evafile.html#evametoc.evafile.eva_to_dir","title":"<code>eva_to_dir(input_eva_file, output_dir)</code>","text":"<p>Compresses a dir into a zip renamed to a .eva file</p> <p>Parameters:</p> <ul> <li> <code>input_eva_file</code>               (<code>str</code>)           \u2013            <p>Path to .eva file</p> </li> <li> <code>output_dir</code>               (<code>str</code>)           \u2013            <p>output dir</p> </li> </ul> Source code in <code>package/src/evametoc/evafile.py</code> <pre><code>def eva_to_dir(input_eva_file, output_dir):\n    \"\"\"Compresses a dir into a zip renamed to a .eva file\n\n    Args:\n        input_eva_file (str): Path to .eva file\n        output_dir (str): output dir\n    \"\"\"\n    shutil.unpack_archive(input_eva_file, output_dir, 'zip')\n</code></pre>"},{"location":"reference/evametoc/metadata.html","title":"metadata","text":"<p>This module provides routines to read and write the metadata alongside the video files</p>"},{"location":"reference/evametoc/metadata.html#evametoc.metadata.NumpyJsonDecoder","title":"<code>NumpyJsonDecoder(*args, **kwargs)</code>","text":"<p>               Bases: <code>JSONDecoder</code></p> <p>Extension of the default json.JSONDecoder that convert strings back to datetime.datetime-s</p> Source code in <code>package/src/evametoc/metadata.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    super(NumpyJsonDecoder, self).__init__(*args, **kwargs)\n    self.parse_string = self.parse_string_or_date\n    self.scan_once = json.decoder.scanner.py_make_scanner(self)\n    self.dtregex = re.compile(self.datetime_regex_str)\n</code></pre>"},{"location":"reference/evametoc/metadata.html#evametoc.metadata.NumpyJsonDecoder.parse_string_or_date","title":"<code>parse_string_or_date(s, *args, **kwargs)</code>","text":"<p>Replacement for the default parse_string method. Looks for strings that matches the self.dtregex</p> Source code in <code>package/src/evametoc/metadata.py</code> <pre><code>def parse_string_or_date(self, s, *args, **kwargs):\n    \"\"\"Replacement for the default parse_string method. Looks for strings that matches the self.dtregex\"\"\"\n    s, end = json.decoder.scanstring(s, *args, **kwargs)  # Call to the original method\n    if self.dtregex.match(s):\n        # If it looks like a date, it is probably a date. Try to decode\n        try:\n            s = dateutil.parser.parse(s)\n        except (dateutil.parser.ParserError, dateutil.parser.UnknownTimezoneWarning):\n            pass\n    return s, end\n</code></pre>"},{"location":"reference/evametoc/metadata.html#evametoc.metadata.NumpyJsonEncoder","title":"<code>NumpyJsonEncoder</code>","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>Extension to the default JSON encoder that also encode numpy.arrays, numpy.floats and numpy.datetime64[ns]</p> Numpy Python JSON ndarray list array float32 float number float64 float number datetime64[ns] string string - datetime.datetime string <p>Datetimes are encoded as %Y-%m-%dT%H:%M:%S%z. Other data types (not defined above) are encoded using the regular json.JSONEncoder</p> <p>The numpy module does not provide this functionality, so it is included here</p>"},{"location":"reference/evametoc/metadata.html#evametoc.metadata.NumpyJsonEncoder.default","title":"<code>default(obj)</code>","text":"<p>This method returns a serializable object for <code>obj</code></p> Source code in <code>package/src/evametoc/metadata.py</code> <pre><code>def default(self, obj):\n    \"\"\"This method returns a serializable object for ``obj``\"\"\"\n    if isinstance(obj, np.ndarray):\n        if 'datetime64[ns]' in obj.dtype.name:\n            # Datetimes are encoded as int in ns, but that notation is unintuitive and\n            # that precision is rarely needed\n            return obj.astype('datetime64[s]').astype('str').tolist()\n        else:\n            return obj.tolist()\n    if np.issubdtype(obj, np.floating):\n        return float(obj)\n    if np.issubdtype(obj, np.integer):\n        return int(obj)\n    if isinstance(obj, datetime.datetime):\n        return obj.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n    return json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"reference/evametoc/metadata.html#evametoc.metadata.find","title":"<code>find(video_file)</code>","text":"<p>Searches for the metadata file in the dir of a video file</p> <p>Parameters:</p> <ul> <li> <code>video_file</code>               (<code>str</code>)           \u2013            <p>path of the video file, for which the metadata must be found.</p> </li> </ul> <p>Returns:     metafile (str): path of the metadata-file Raises:     OSError: if the file could not be found</p> Source code in <code>package/src/evametoc/metadata.py</code> <pre><code>def find(video_file):\n    \"\"\"Searches for the metadata file in the dir of a video file\n\n    Args:\n        video_file (str): path of the video file, for which the metadata must be found.\n    Returns:\n        metafile (str): path of the metadata-file\n    Raises:\n        OSError: if the file could not be found\n    \"\"\"\n    if os.path.isfile(video_file + '.json'):\n        metafile = video_file + '.json'\n        return metafile\n    else:\n        metafile = os.path.join(os.path.dirname(video_file), 'meta.json')\n        if os.path.isfile(metafile):\n            with open(metafile, 'r') as fh:\n                metadata = json.load(fh, cls=NumpyJsonDecoder)\n            if os.path.basename(video_file) in metadata:\n                return metafile\n            raise OSError(61, \"No data available\", metafile)\n        else:\n            raise OSError(2, \"No such file or directory\", metafile)\n</code></pre>"},{"location":"reference/evametoc/metadata.html#evametoc.metadata.read","title":"<code>read(video_file, separate_metadata=None)</code>","text":"<p>Reads the metadata for a certain video file</p> <p>Parameters:</p> <ul> <li> <code>video_file</code>               (<code>str</code>)           \u2013            <p>path to the video file, for which the metadata must be read.</p> </li> <li> <code>separate_metadata</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>if true, assumes separate metadata files for each video, if false, assumes all metadata in one meta.json file, if None, searches for the metadata-file using find_metadata</p> </li> </ul> <p>Returns:     metadata (dict): metadata for this video file     metaspec (dict): global metadata (such as shared_coords) present in the metadata-file</p> Source code in <code>package/src/evametoc/metadata.py</code> <pre><code>def read(video_file, separate_metadata=None):\n    \"\"\"Reads the metadata for a certain video file\n\n    Args:\n        video_file (str): path to the video file, for which the metadata must be read.\n        separate_metadata (bool): if true, assumes separate metadata files for each video,\n            if false, assumes all metadata in one meta.json file,\n            if None, searches for the metadata-file using find_metadata\n    Returns:\n        metadata (dict): metadata for this video file\n        metaspec (dict): global metadata (such as _shared_coords_) present in the metadata-file\n    \"\"\"\n    # Define metadata-filename\n    if separate_metadata is None:\n        metafile = find(video_file)\n    elif separate_metadata:\n        metafile = video_file + '.json'\n    else:\n        metafile = os.path.join(os.path.dirname(video_file), 'meta.json')\n\n    # Open the metadata\n    with open(metafile, 'r') as fh:\n        metadata = json.load(fh, cls=NumpyJsonDecoder)\n\n    # Extract the video file-specific and global metadata\n    metaspec = {key: v for key, v in metadata.items() if key.startswith('_') and key.endswith('_')}\n    return metadata[os.path.basename(video_file)], metaspec\n</code></pre>"},{"location":"reference/evametoc/metadata.html#evametoc.metadata.write","title":"<code>write(video_file, metadata, separate_metadata=False)</code>","text":"<p>Writes/appends the metadata for one video file to a json-file.</p> <p>Writes the metadata for one video file to a json file, in the same folder as the video file. This JSON-file contains the scaling data for the video file, and other data needed for recreation of the original data. If a meta.json already exists, the data will be appended.</p> <p>Parameters:</p> <ul> <li> <code>video_file</code>               (<code>str</code>)           \u2013            <p>Path to the video file, in which folder the metadata will be placed.</p> </li> <li> <code>metadata</code>               (<code>dict</code>)           \u2013            <p>Meta-data for this video file.</p> </li> <li> <code>separate_metadata</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>if true, uses separate metadata files for each video, if false, writes all metadata to one meta.json file.</p> </li> </ul> Source code in <code>package/src/evametoc/metadata.py</code> <pre><code>def write(video_file, metadata, separate_metadata=False):\n    \"\"\"Writes/appends the metadata for one video file to a json-file.\n\n    Writes the metadata for one video file to a json file, in the same folder as the video file.\n    This JSON-file contains the scaling data for the video file, and other data needed\n    for recreation of the original data. If a meta.json already exists, the data will be appended.\n\n    Args:\n        video_file (str): Path to the video file, in which folder the metadata will be placed.\n        metadata (dict): Meta-data for this video file.\n        separate_metadata (bool): if true, uses separate metadata files for each video,\n            if false, writes all metadata to one meta.json file.\n    \"\"\"\n    assert all(k in metadata.keys() for k in \"BGR\"), \"Metadata must have keys B, G or R\"\n\n    # Check for any special/root metadata-keys, like _shared_coords_\n    metadata_special = {}\n    metadata_video = {}\n    for key in list(metadata.keys()):\n        if key.startswith('_') and key.endswith('_'):\n            metadata_special[key] = metadata[key]\n        else:\n            metadata_video[key] = metadata[key]\n\n    # Get path of the metadata-file\n    metafile = os.path.join(os.path.dirname(video_file), 'meta.json')\n    if separate_metadata:\n        metafile = video_file + '.json'\n\n    # Generate the dict to be added to the metadata-file\n    meta_new = {os.path.basename(video_file): metadata_video}\n    meta_new.update(metadata_special)\n\n    # Write the metadata to file, or join with existing metadata\n    if not os.path.exists(metafile):\n        with open(metafile, 'w') as fh:\n            json.dump(meta_new, fh, cls=NumpyJsonEncoder, indent=4)\n    else:\n        with open(metafile, 'r+') as fh:\n            meta = json.load(fh, cls=NumpyJsonDecoder)\n            fh.seek(0)\n\n            # Make sure new video-files are added, and specials (like coords) are updated\n            for key in set(list(meta.keys()) + list(meta_new.keys())):\n                if key not in meta:\n                    meta[key] = meta_new[key]\n                elif key in meta_new:\n                    if key.startswith('_') and key.endswith('_'):\n                        meta[key].update(meta_new[key])\n                    else:\n                        meta[key] = meta_new[key]\n            json.dump(meta, fh, cls=NumpyJsonEncoder, indent=4)\n</code></pre>"},{"location":"reference/evametoc/utils.html","title":"utils","text":"<p>Module to provide some generic functions to this package.</p>"},{"location":"reference/evametoc/utils.html#evametoc.utils.find_target_path","title":"<code>find_target_path(source_path, suffix, target_path=None, overwrite=False)</code>","text":"<p>Generates a path where to store a file, based on a source-file path.</p> <p>Parameters:</p> <ul> <li> <code>source_path</code>               (<code>str</code>)           \u2013            <p>path of the source file</p> </li> <li> <code>suffix</code>               (<code>str</code>)           \u2013            <p>extension (with suffix) of the target file</p> </li> <li> <code>target_path</code>               (<code>str or None</code>, default:                   <code>None</code> )           \u2013            <p>optional path that a user may have supplied. If not None, uses this path instead of source_path and suffix</p> </li> <li> <code>overwrite</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Overwrite an existing file If False, appends the target_path with a number</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>target_path</code> (              <code>str</code> )          \u2013            <p>a location where the target file may be written.</p> </li> </ul> Source code in <code>package/src/evametoc/utils.py</code> <pre><code>def find_target_path(source_path, suffix, target_path=None, overwrite=False):\n    \"\"\"Generates a path where to store a file, based on a source-file path.\n\n    Args:\n        source_path (str): path of the source file\n        suffix (str): extension (with suffix) of the target file\n        target_path (str or None): optional path that a user may have supplied.\n            If not None, uses this path instead of source_path and suffix\n        overwrite (bool): Overwrite an existing file\n            If False, appends the target_path with a number\n\n    Returns:\n        target_path (str): a location where the target file may be written.\n    \"\"\"\n    if target_path is not None:\n        return target_path\n\n    source_path_root, source_path_ext = os.path.splitext(source_path)\n    target_path = source_path_root + suffix\n\n    if not os.path.exists(target_path):\n        return target_path\n    if overwrite:\n        return target_path\n\n    suffix_start, suffix_ext = suffix.rsplit('.', 1)\n    existing_files = glob.glob(source_path_root + suffix_start + '.*.' + suffix_ext)\n    existing_files_numbers = list(map(lambda fn: int(fn.rsplit('.', 3)[-2]), existing_files))\n    if existing_files_numbers:\n        new_number = max(existing_files_numbers) + 1\n        return source_path_root + suffix_start + f'.{new_number:d}.' + suffix_ext\n    else:\n        return source_path_root + suffix_start + '.1.' + suffix_ext\n</code></pre>"},{"location":"reference/evametoc/utils.html#evametoc.utils.human_readable_file_size","title":"<code>human_readable_file_size(num, suffix='B')</code>","text":"<p>Writes a number of bytes to a human-readable string</p> <p>Parameters:</p> <ul> <li> <code>num</code>               (<code>int</code>)           \u2013            <p>Number of bytes</p> </li> <li> <code>suffix</code>               (<code>string</code>, default:                   <code>'B'</code> )           \u2013            <p>Unit to be placed after the suffix (default is \"B\" for bytes)</p> </li> </ul> <p>Returns:     human_readable (string): Number of bytes in a human-readable format (e.g. 154.7GiB)</p> Source code in <code>package/src/evametoc/utils.py</code> <pre><code>def human_readable_file_size(num, suffix=\"B\"):\n    \"\"\"Writes a number of bytes to a human-readable string\n\n    Args:\n        num (int): Number of bytes\n        suffix (string): Unit to be placed after the suffix (default is \"B\" for bytes)\n    Returns:\n        human_readable (string): Number of bytes in a human-readable format (e.g. 154.7GiB)\n    \"\"\"\n    if num is None:\n        return \"Unconstrained\"\n    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) &lt; 1024.0:\n            return f\"{num:3.1f}{unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f}Yi{suffix}\"\n</code></pre>"},{"location":"reference/evametoc/utils.html#evametoc.utils.sort_dict_by_keys","title":"<code>sort_dict_by_keys(dictionary)</code>","text":"<p>Sorts a dictionary by its keys</p> <p>Parameters:</p> <ul> <li> <code>dictionary</code>               (<code>dict</code>)           \u2013            <p>input dictionary</p> </li> </ul> <p>Returns:     sorted (dict): a copy of dictionary sorted by its keys</p> Source code in <code>package/src/evametoc/utils.py</code> <pre><code>def sort_dict_by_keys(dictionary):\n    \"\"\"Sorts a dictionary by its keys\n\n    Args:\n        dictionary (dict): input dictionary\n    Returns:\n        sorted (dict): a copy of dictionary sorted by its keys\n    \"\"\"\n    return dict(sorted(dictionary.items(), key=lambda d: d[0]))\n</code></pre>"},{"location":"reference/evametoc/data/group.html","title":"group","text":"<p>This module provides grouping functions, that group the parameters in sets of 3, to be encoded into one video.</p>"},{"location":"reference/evametoc/data/group.html#evametoc.data.group.by_unit","title":"<code>by_unit(dataset)</code>","text":"<p>Creates groups of three variables to be stored in one video (Blue,Green,Red), based on their unit</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>A dataset containing all variables</p> </li> </ul> <p>Returns:     groups (list&lt;list&lt;3 x string&gt;&gt;): List of groups of 3 keys that could be stored together in one video file</p> Source code in <code>package/src/evametoc/data/group.py</code> <pre><code>def by_unit(dataset):\n    \"\"\"Creates groups of three variables to be stored in one video (Blue,Green,Red), based on their unit\n\n    Args:\n        dataset (xarray.Dataset): A dataset containing all variables\n    Returns:\n        groups (list&lt;list&lt;3 x string&gt;&gt;): List of groups of 3 keys that could be stored together in one video file\n    \"\"\"\n    params = list(dataset.keys())\n    if len(params) % 3 == 1:\n        print(\"Number of fields in the dataset are not dividable by three!\")\n        print(\"Dropping:\", params.pop())\n    elif len(params) % 3 == 2:\n        print(\"Number of fields in the dataset are not dividable by three!\")\n        print(\"Dropping:\", params.pop(), params.pop())\n    units = {param: dataset[param].attrs.get('units', '-') for param in params}\n    param_levels = {param: param.split('|') for param in params}\n    params_sorted = sorted(params,\n                           key=lambda param_name: (units[param_name], *param_levels[param_name]))\n    params_grouped = list(zip(params_sorted[0::3], params_sorted[1::3], params_sorted[2::3]))\n    return params_grouped\n</code></pre>"},{"location":"reference/evametoc/data/group.html#evametoc.data.group.group_random","title":"<code>group_random(dataset)</code>","text":"<p>Creates groups of three variables to be stored in one video (Blue,Green,Red), randomly</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>A dataset containing all variables</p> </li> </ul> <p>Returns:     groups (list&lt;list&lt;3 x string&gt;&gt;): List of groups of 3 keys that could be stored together in one video file</p> Source code in <code>package/src/evametoc/data/group.py</code> <pre><code>def group_random(dataset):\n    \"\"\"Creates groups of three variables to be stored in one video (Blue,Green,Red), randomly\n\n    Args:\n        dataset (xarray.Dataset): A dataset containing all variables\n    Returns:\n        groups (list&lt;list&lt;3 x string&gt;&gt;): List of groups of 3 keys that could be stored together in one video file\n    \"\"\"\n    params = list(dataset.keys())\n    if len(params) % 3 == 1:\n        print(\"Number of fields in the dataset are not dividable by three!\")\n        print(\"Dropping:\", params.pop())\n    elif len(params) % 3 == 2:\n        print(\"Number of fields in the dataset are not dividable by three!\")\n        print(\"Dropping:\", params.pop(), params.pop())\n    random.shuffle(params)\n    params_grouped = list(zip(params[0::3], params[1::3], params[2::3]))\n    return params_grouped\n</code></pre>"},{"location":"reference/evametoc/data/nans.html","title":"nans","text":"<p>This module provides functions for extracting, compressing, filling, decompressing and resetting Nan values in an environmental dataset.</p>"},{"location":"reference/evametoc/data/nans.html#evametoc.data.nans.decode_nan","title":"<code>decode_nan(nancoded)</code>","text":"<p>Decode a nan-object</p> <p>Returns:     nanarr (np.array[bool]): True where a NaN value was present, False otherwise</p> Source code in <code>package/src/evametoc/data/nans.py</code> <pre><code>def decode_nan(nancoded):\n    \"\"\"Decode a nan-object\n\n    Args:\n        nancoded (dict)\n            - compression (str): Method used for compression, choose from ['base64','lzma']\n            - data (str): base64 encoded nan-positions, compressed with algoritm\n            - shape (list): shape of the original array\n    Returns:\n        nanarr (np.array[bool]): True where a NaN value was present, False otherwise\n    \"\"\"\n    assert all([k in nancoded for k in ['compression', 'data', 'shape']])\n    assert nancoded['compression'] in ['base64', 'lzma']\n\n    # Get the data\n    nanbstr = base64.b64decode(nancoded['data'])\n\n    # Decompress using the original compression lib\n    if nancoded['compression'] == 'lzma':\n        nanbstr = lzma.decompress(nanbstr)\n\n    # Create a numpy.array from the compressed bytes and go from bytes to bits.\n    nanbyte = np.frombuffer(nanbstr, dtype=np.uint8)\n    nanbits = np.unpackbits(nanbyte) == 1\n\n    # Reshape to the original shape\n    if nanbits.size &lt; np.prod(nancoded['shape']):\n        nanarr = reshape_bits(nanbits, nancoded['shape'][1:])\n        return np.repeat(nanarr[None, ...], nancoded['shape'][0], axis=0)\n    return reshape_bits(nanbits, nancoded['shape'])\n</code></pre>"},{"location":"reference/evametoc/data/nans.html#evametoc.data.nans.encode_nan","title":"<code>encode_nan(data, algorithm='lzma')</code>","text":"<p>Encode the nan's in data using a lossless compression algorithm</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>array</code>)           \u2013            <p>The dataset containing NaNs</p> </li> <li> <code>algorithm</code>               (<code>str</code>, default:                   <code>'lzma'</code> )           \u2013            <p>Method used for compression, choose from [None,'base64','lzma']</p> </li> </ul> <p>Returns:     nanobj (dict):         - compression (str): The algorithm used         - data (str): base64 encoded nan-positions, compressed with algorithm         - shape (list): shape of the original array</p> Source code in <code>package/src/evametoc/data/nans.py</code> <pre><code>def encode_nan(data, algorithm='lzma'):\n    \"\"\"Encode the nan's in data using a lossless compression algorithm\n\n    Args:\n        data (np.array): The dataset containing NaNs\n        algorithm (str): Method used for compression, choose from [None,'base64','lzma']\n    Returns:\n        nanobj (dict):\n            - compression (str): The algorithm used\n            - data (str): base64 encoded nan-positions, compressed with algorithm\n            - shape (list): shape of the original array\n    \"\"\"\n    assert algorithm in [None, 'best', 'base64', 'lzma']\n    algorithm = 'best' if algorithm is None else algorithm\n    algorithm = 'lzma' if algorithm == 'best' else algorithm\n\n    nanfield = np.isnan(data)\n\n    # Often nan-s encode lack of data due to ground level\n    # and are therefore time-independent. Remove time dimension if so...\n    equal_over_axis0 = np.equal.reduce(nanfield, axis=0).all()\n    if equal_over_axis0:\n        nanflat = nanfield[0, ...].flatten()\n    else:\n        nanflat = nanfield.flatten()\n\n    # Combine every 8 booleans (NaN Yes/No) to one byte.\n    nanbyte = np.packbits(nanflat).tobytes()\n\n    # Compress the byte-string using one of the python-standard compression libs\n    # (All byte-strings are encoded to base64 for better support in JSON later on...)\n    compressed = {}\n    if algorithm == 'base64':\n        compressed['base64'] = base64.b64encode(nanbyte).decode()\n    elif algorithm == 'lzma':\n        nanlzma = lzma.compress(nanbyte)\n        compressed['lzma'] = base64.b64encode(nanlzma).decode()\n\n    # Return a nan-object which can be implemented in the JSON file\n    return {'compression': algorithm,\n            'data': compressed[algorithm],\n            'shape': nanfield.shape}\n</code></pre>"},{"location":"reference/evametoc/data/nans.html#evametoc.data.nans.fill_nans","title":"<code>fill_nans(dataset)</code>","text":"<p>Fill the NaNs in the dataset</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>array</code>)           \u2013            <p>Dataset containing nans, minimal 2 dims.</p> </li> </ul> <p>Returns:     filled (np.array): Dataset containg all values from <code>dataset</code>, with all nans filled</p> Source code in <code>package/src/evametoc/data/nans.py</code> <pre><code>def fill_nans(dataset):\n    \"\"\"Fill the NaNs in the dataset\n\n    Args:\n        dataset (np.array): Dataset containing nans, minimal 2 dims.\n    Returns:\n        filled (np.array): Dataset containg all values from `dataset`, with all nans filled\n    \"\"\"\n    if dataset.ndim &gt; 2:\n        # This function only works for 2d-slices, so slice this dimension to 2D\n        filled = []\n        for timestep in range(dataset.shape[0]):\n            filled.append(fill_nans(dataset[timestep, ...])[None, ...])\n        return np.concatenate(filled, axis=0)\n    elif dataset.ndim == 2:\n        # Get the NaN positions\n        nans = np.isnan(dataset)\n\n        # For each NaN, take the nearest neighbor\n        nearest_index = scipy.ndimage.distance_transform_edt(\n            nans, return_distances=False, return_indices=True)\n        nearest_neighbor = dataset[tuple(nearest_index)]\n\n        # Apply a gausian blur, so that the resulting image is more smooth\n        gausian_blur = scipy.ndimage.gaussian_filter(nearest_neighbor, 5)\n\n        # Make sure no datapoints were alterated during this process\n        return np.where(nans, gausian_blur, dataset)\n    else:\n        raise ValueError(\"Not enough dimensions for 2D fill\")\n</code></pre>"},{"location":"reference/evametoc/data/nans.html#evametoc.data.nans.reshape_bits","title":"<code>reshape_bits(bitsarray, target_shape, *args, **kwargs)</code>","text":"<p>Reshapes a bitarray back to its original shape</p> <p>When bits are converted to bytes, using np.packbits(nanflat) in encode_nan, they are padded with upto 7 bits to generate a number of bytes. This function corrects for that padding, and reshapes to the right target shape by removing the last few bits</p> <p>Parameters:</p> <ul> <li> <code>bitsarray</code>               (<code>array[bool]</code>)           \u2013            <p>1D array of bits</p> </li> <li> <code>target_shape</code>               (<code>list[int]</code>)           \u2013            <p>shape of the target array</p> </li> <li> <code>*args</code>               (<code>and **kwargs</code>, default:                   <code>()</code> )           \u2013            <p>passed to np.reshape</p> </li> </ul> <p>Returns:     bitsarray (np.array[bool]): Array with dimensions target_shape, containing the bits</p> Source code in <code>package/src/evametoc/data/nans.py</code> <pre><code>def reshape_bits(bitsarray, target_shape, *args, **kwargs):\n    \"\"\"Reshapes a bitarray back to its original shape\n\n    When bits are converted to bytes, using np.packbits(nanflat) in encode_nan,\n    they are padded with upto 7 bits to generate a number of bytes. This function\n    corrects for that padding, and reshapes to the right target shape by removing\n    the last few bits\n\n    Args:\n        bitsarray (np.array[bool]): 1D array of bits\n        target_shape (list[int]): shape of the target array\n        *args and **kwargs: passed to np.reshape\n    Returns:\n        bitsarray (np.array[bool]): Array with dimensions target_shape, containing the bits\n    \"\"\"\n    assert bitsarray.size &gt;= np.prod(target_shape)\n    assert bitsarray.size &lt; np.prod(target_shape) + 8 # Another problem might exist\n    if bitsarray.size &gt; np.prod(target_shape):\n        assert bitsarray[np.prod(target_shape):].any() == False  # Content discoverd, this is not a correct solution\n        bitsarray = bitsarray[:np.prod(target_shape)]\n    return np.reshape(bitsarray, target_shape, *args, **kwargs)\n</code></pre>"},{"location":"reference/evametoc/data/nans.html#evametoc.data.nans.set_nan","title":"<code>set_nan(data, nanarr)</code>","text":"<p>Sets data to np.nan where nanarr is True</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>array</code>)           \u2013            <p>Array with values</p> </li> <li> <code>nanarr</code>               (<code>array</code>)           \u2013            <p>Boolean array, True where NaN, having the same shape as data</p> </li> </ul> <p>Returns:     arr (np.array):</p> Source code in <code>package/src/evametoc/data/nans.py</code> <pre><code>def set_nan(data, nanarr):\n    \"\"\"Sets data to np.nan where nanarr is True\n\n    Args:\n        data (np.array): Array with values\n        nanarr (np.array): Boolean array, True where NaN, having the same shape as data\n    Returns:\n        arr (np.array):\n    \"\"\"\n    assert data.shape == nanarr.shape\n    arr = data.copy()\n    arr[nanarr] = np.nan\n    return arr\n</code></pre>"},{"location":"reference/evametoc/data/postprocess.html","title":"postprocess","text":"<p>This module provides some postprocessing functions, that need to be applied after data has been extracted from a video file.</p> <p>Reverse functions from preprocess.py Video files only support 3-dimensions: Width, Height and Time, whereas the original NetCDF could have had 4 or more dimensions. Therefore, the output data needs some postprocessing.</p>"},{"location":"reference/evametoc/data/postprocess.html#evametoc.data.postprocess.merge_levels","title":"<code>merge_levels(dataset)</code>","text":"<p>Merges levels, were they were separated before across multiple vars in the dataset Undoes what preprocess.split_levels does</p> <p>Variables containing \"|z\" will be merged. The name of the dimension will be retrieved from the attribute starting with \"z\", as will the value within that dimension.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>A xarray.dataset with dimensions [time,levels,width,height]</p> </li> </ul> <p>Returns:     dataset (xarray.Dataset): A xarray.dataset with dimensions [time,width,height]</p> Source code in <code>package/src/evametoc/data/postprocess.py</code> <pre><code>def merge_levels(dataset):\n    \"\"\"Merges levels, were they were separated before across multiple vars in the dataset\n    Undoes what preprocess.split_levels does\n\n    Variables containing \"|z\" will be merged. The name of the dimension will be retrieved from the\n    attribute starting with \"_z_\", as will the value within that dimension.\n\n    Args:\n        dataset (xarray.Dataset): A xarray.dataset with dimensions [time,levels,width,height]\n    Returns:\n        dataset (xarray.Dataset): A xarray.dataset with dimensions [time,width,height]\n    \"\"\"\n    result = dataset.copy()\n    restored_leveled_data = {}\n    restored_leveled_dims = {}\n    vars_to_be_dropped = []\n\n    for k in list(result.keys()):\n        if '|z' in k:\n            # Parameter name contains |z, so it is a level from a separated variable\n            # Get the name of the level dimension, its value within that dimension,\n            # name of the original var and level-index\n            data_array = result[k]\n            param_name, level = k.split('|z')\n            level = int(level)\n            z_dim_name, z_dim_value = 'z', level\n            for attr in data_array.attrs.keys():\n                if attr.startswith('_z_'):\n                    z_dim_name, z_dim_value = attr[3:], data_array.attrs[attr]\n                    data_array.attrs.pop(attr)\n                    break\n\n            # Store the level-index/value, so we can rebuild this dimension later on\n            if z_dim_name not in restored_leveled_dims:\n                restored_leveled_dims[z_dim_name] = {}\n            restored_leveled_dims[z_dim_name][level] = z_dim_value\n\n            # Store the dimensions, data and attributes, so we can rebuild the variable\n            if param_name not in restored_leveled_data:\n                dims = [data_array.dims[0], z_dim_name] + list(data_array.dims[1:])\n                restored_leveled_data[param_name] = {'dims': dims, 'levels': {}, 'attrs': {}}\n            restored_leveled_data[param_name]['attrs'].update(data_array.attrs)\n            restored_leveled_data[param_name]['levels'][level] = data_array.values[:, np.newaxis, :, :]\n\n            # Schedule the separated var to be deleted\n            vars_to_be_dropped.append(k)\n\n    # Order the dimensions by level-index, and alphabetically\n    for k in sorted(restored_leveled_dims):\n        restored_leveled_dims[k] = list(evametoc.utils.sort_dict_by_keys(restored_leveled_dims[k]).values())\n    result = result.assign_coords(**restored_leveled_dims)\n\n    # Add the new merged variables\n    for param_name in sorted(restored_leveled_data):\n        data = []\n        for level in sorted(restored_leveled_data[param_name]['levels']):\n            data.append(restored_leveled_data[param_name]['levels'][level])\n        data = np.concatenate(data, axis=1)\n        result = result.assign(**{param_name: (\n            restored_leveled_data[param_name]['dims'],\n            data,\n            restored_leveled_data[param_name]['attrs']\n        )})\n    # delete the old ones\n    result = result.drop_vars(vars_to_be_dropped)\n\n    return result\n</code></pre>"},{"location":"reference/evametoc/data/postprocess.html#evametoc.data.postprocess.readd_no_data_params","title":"<code>readd_no_data_params(dataset, metaspec)</code>","text":"<p>Readds parameters that were removed for not containing any data, or have a constant value Undoes what preprocess.remove_no_data_params does</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset with parameters removed</p> </li> <li> <code>metaspec</code>               (<code>dict</code>)           \u2013            <p>The global metadata (such as shared_coords) present in the metadata-file</p> </li> </ul> <p>Returns:     dataset (xarray.Dataset): A copy of dataset with those parameters readded</p> Source code in <code>package/src/evametoc/data/postprocess.py</code> <pre><code>def readd_no_data_params(dataset, metaspec):\n    \"\"\"Readds parameters that were removed for not containing any data, or have a constant value\n    Undoes what preprocess.remove_no_data_params does\n\n    Args:\n        dataset (xarray.Dataset): The dataset with parameters removed\n        metaspec (dict): The global metadata (such as _shared_coords_) present in the metadata-file\n    Returns:\n        dataset (xarray.Dataset): A copy of dataset with those parameters readded\n    \"\"\"\n    if '_no_data_vars_' not in metaspec:\n        return dataset\n    if not metaspec['_no_data_vars_']:\n        return dataset\n\n    ds = dataset.copy()\n\n    first_xarray = ds[list(ds.keys())[0]]\n    for param_name, properties in metaspec['_no_data_vars_'].items():\n        value = properties.get('value', 'nan')\n        dims = properties.get('dims', first_xarray.dims)\n        shape = properties.get('shape', first_xarray.values.shape)\n        attrs = properties.get('attrs', {})\n        if value == 'nan':\n            value = np.nan\n        ds = ds.assign(**{param_name: (dims, np.full(shape, value), attrs)})\n    return ds\n</code></pre>"},{"location":"reference/evametoc/data/prepare.html","title":"prepare","text":"<p>This module provides the file-type specific preparation functions for this package</p> <p>Some files (like harmonie-files) contain accumulated data, nan's, stacked data or static data. This module provides functions to handle those datasets, and prepares them for use in this package</p>"},{"location":"reference/evametoc/data/prepare.html#evametoc.data.prepare.differentiate_array","title":"<code>differentiate_array(accumulated_data)</code>","text":"<p>Some arrays have summed fields (over time). This method reverses that.</p> <p>Parameters:</p> <ul> <li> <code>accumulated_data</code>               (<code>DataArray</code>)           \u2013            <p>A xarray with the first dimension time, which contains sums over time. (For example: accumulated rain or radiation since start of this model run).</p> </li> </ul> <p>Returns:     data (xarray.DataArray): A xarray containing the differences between timesteps of accumulated_data</p> Source code in <code>package/src/evametoc/data/prepare.py</code> <pre><code>def differentiate_array(accumulated_data):\n    \"\"\"Some arrays have summed fields (over time). This method reverses that.\n\n    Args:\n        accumulated_data (xarray.DataArray): A xarray with the first dimension time, which contains sums\n            over time. (For example: accumulated rain or radiation since start of this model run).\n    Returns:\n        data (xarray.DataArray): A xarray containing the differences between timesteps of accumulated_data\n    \"\"\"\n    # Essentially np.diff + a zero array in the first timestep...\n    dc = np.zeros(accumulated_data.shape)\n    dc[0, ...] = accumulated_data[0, ...]\n    dc[1:, ...] = np.diff(accumulated_data.values, axis=0)\n    return xr.DataArray(dc, coords=accumulated_data.coords, name=accumulated_data.name, attrs=accumulated_data.attrs)\n</code></pre>"},{"location":"reference/evametoc/data/prepare.html#evametoc.data.prepare.differentiation_knmi","title":"<code>differentiation_knmi(dataset)</code>","text":"<p>Differentiates accumulated fields in a KNMI dataset (e.g. rain, radiation)</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>A KNMI dataset that may contain accumulated fields</p> </li> </ul> <p>Returns:     dataset (xarray.Dataset): The input dataset, with the accumulated fields, differentiated</p> Source code in <code>package/src/evametoc/data/prepare.py</code> <pre><code>def differentiation_knmi(dataset):\n    \"\"\"Differentiates accumulated fields in a KNMI dataset (e.g. rain, radiation)\n\n    Args:\n        dataset (xarray.Dataset): A KNMI dataset that may contain accumulated fields\n    Returns:\n        dataset (xarray.Dataset): The input dataset, with the accumulated fields, differentiated\n    \"\"\"\n    ds = dataset.copy()\n    for k in ds.keys():\n        table = ds[k].attrs.get('table', 0)\n        pcode = ds[k].attrs.get('code', 0)\n        if table in [253] and pcode in [111, 112, 117, 122, 132, 181, 184, 201]:\n            # These variables are cumulated according to the KNMI Grib table (253)\n            # https://www.knmidata.nl/data-services/knmi-producten-overzicht/atmosfeer-modeldata/data-product-1\n            ds[k] = differentiate_array(ds[k])\n            ds[k].attrs['_aggr'] = 'sum'\n    return ds\n</code></pre>"},{"location":"reference/evametoc/data/prepare.html#evametoc.data.prepare.drop_knmi_landsea","title":"<code>drop_knmi_landsea(dataset)</code>","text":"<p>Drops the land-sea mask from KNMI datasets</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>A KNMI dataset that may contain a landsea mask</p> </li> </ul> <p>Returns:     dataset (xarray.Dataset): The input dataset, without KNMI landsea mask</p> Source code in <code>package/src/evametoc/data/prepare.py</code> <pre><code>def drop_knmi_landsea(dataset):\n    \"\"\"Drops the land-sea mask from KNMI datasets\n\n    Args:\n        dataset (xarray.Dataset): A KNMI dataset that may contain a landsea mask\n    Returns:\n        dataset (xarray.Dataset): The input dataset, without KNMI landsea mask\n    \"\"\"\n    ds = dataset.copy()\n    for k in ds.keys():\n        table = ds[k].attrs.get('table', 0)\n        pcode = ds[k].attrs.get('code', 0)\n        if table in [253] and pcode in [81]:\n            # Land-sea mask (81) according to KNMI Grib table (253)\n            ds = ds.drop_vars([k])\n    return ds\n</code></pre>"},{"location":"reference/evametoc/data/prepare.html#evametoc.data.prepare.prepare","title":"<code>prepare(dataset, dataset_type=None)</code>","text":"<p>Finds and applies the relevant preparation functions</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>The dataset to be prepared.</p> </li> <li> <code>dataset_type</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The type of dataset to be prepared. One of ['harm40','GFS','copernicus_ocean', None]. If no valid dataset_type was selected, no preparation is applied.</p> </li> </ul> <p>Returns:     prepared_dataset (xarray.Dataset): The dataset with relevant preparation functions applied</p> Source code in <code>package/src/evametoc/data/prepare.py</code> <pre><code>def prepare(dataset, dataset_type=None):\n    \"\"\"Finds and applies the relevant preparation functions\n\n    Args:\n        dataset (xarray.Dataset): The dataset to be prepared.\n        dataset_type (str): The type of dataset to be prepared.\n            One of ['harm40','GFS','copernicus_ocean', None].\n            If no valid dataset_type was selected, no preparation is applied.\n    Returns:\n        prepared_dataset (xarray.Dataset): The dataset with relevant preparation functions applied\n    \"\"\"\n    prepare_functions = {\n        'harm40': [drop_knmi_landsea, differentiation_knmi],\n        'GFS': [],\n        'copernicus_ocean': [],\n        None: [],\n    }\n\n    steps = prepare_functions.get(dataset_type, None)\n    if steps is None:\n        logging.warning(f'The dataset_type \"{dataset_type:s}\" was unknown. No preparation applied.')\n        return dataset\n    dataset = dataset.copy()\n    for step in steps:\n        dataset = step(dataset)\n    return dataset\n</code></pre>"},{"location":"reference/evametoc/data/preprocess.html","title":"preprocess","text":"<p>This module provides some preprocessing functions, that need to be applied before conversion to a video file</p> <p>Video files only support 3-dimensions: Width, Height and Time. Therefore, the input data needs some preprocessing.</p>"},{"location":"reference/evametoc/data/preprocess.html#evametoc.data.preprocess.make_num_params_multi3","title":"<code>make_num_params_multi3(dataset)</code>","text":"<p>Since videos have 3 color channels, we need multitudes of 3 parameters. This function appends dummy parameters to achieve that.</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>An dataset that may contain any number of parameters</p> </li> </ul> <p>Returns:     dataset (xarray.Dataset): An dataset where len(parameters) % 3 == 0</p> Source code in <code>package/src/evametoc/data/preprocess.py</code> <pre><code>def make_num_params_multi3(dataset):\n    \"\"\"Since videos have 3 color channels, we need multitudes of 3 parameters. This function appends dummy parameters to\n    achieve that.\n\n    Args:\n        dataset (xarray.Dataset): An dataset that may contain any number of parameters\n    Returns:\n        dataset (xarray.Dataset): An dataset where len(parameters) % 3 == 0\n    \"\"\"\n    ds = dataset.copy()\n    param_names = list(ds.keys())\n    if len(param_names) % 3 == 2:\n        ds = ds.assign(__dummy1__=(ds[param_names[0]].dims, ds[param_names[0]].values, ds[param_names[0]].attrs))\n    elif len(param_names) % 3 == 1:\n        ds = ds.assign(__dummy1__=(ds[param_names[0]].dims, ds[param_names[0]].values, ds[param_names[0]].attrs),\n                       __dummy2__=(ds[param_names[1]].dims, ds[param_names[1]].values, ds[param_names[1]].attrs))\n    return ds\n</code></pre>"},{"location":"reference/evametoc/data/preprocess.html#evametoc.data.preprocess.remove_no_data_params","title":"<code>remove_no_data_params(dataset, metadata=None)</code>","text":"<p>Removes parameters that do not contain any data, or have a constant value Can be undone by postprocess.readd_no_data_params</p> <p>Parameters:</p> <ul> <li> <code>dataset</code>               (<code>Dataset</code>)           \u2013            <p>An dataset that may contain parameters with a constant value</p> </li> <li> <code>metadata</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>A dictionary containing other metadata that should be kept.</p> </li> </ul> <p>Returns:     dataset (xarray.Dataset): An dataset with those parameters removed     metadata (dict): A dictionary that may be used to rebuild those parameters later on</p> Source code in <code>package/src/evametoc/data/preprocess.py</code> <pre><code>def remove_no_data_params(dataset, metadata=None):\n    \"\"\"Removes parameters that do not contain any data, or have a constant value\n    Can be undone by postprocess.readd_no_data_params\n\n    Args:\n        dataset (xarray.Dataset): An dataset that may contain parameters with a constant value\n        metadata (dict): A dictionary containing other metadata that should be kept.\n    Returns:\n        dataset (xarray.Dataset): An dataset with those parameters removed\n        metadata (dict): A dictionary that may be used to rebuild those parameters later on\n    \"\"\"\n    ds = dataset.copy()\n    if metadata is None:\n        metadata = {'_no_data_vars_': {}}\n    else:\n        metadata = metadata.copy()\n    if '_no_data_vars_' not in metadata:\n        metadata['_no_data_vars_'] = {}\n\n    for param_name in ds.keys():\n        data_xarray = ds[param_name]\n        if np.isnan(data_xarray.values).all():\n            metadata['_no_data_vars_'][param_name] = {'value': 'nan', 'dims': data_xarray.dims,\n                                                      'shape': data_xarray.values.shape, 'attrs': data_xarray.attrs}\n            ds = ds.drop_vars([param_name])\n        elif np.nanmax(data_xarray.values) - np.nanmin(data_xarray.values) == 0:\n            metadata['_no_data_vars_'][param_name] = {'value': np.nanmax(data_xarray.values), 'dims': data_xarray.dims,\n                                                      'shape': data_xarray.values.shape, 'attrs': data_xarray.attrs}\n            ds = ds.drop_vars([param_name])\n    return ds, metadata\n</code></pre>"},{"location":"reference/evametoc/data/preprocess.html#evametoc.data.preprocess.split_levels","title":"<code>split_levels(dataset_with_levels)</code>","text":"<p>Splits levels-dimension in separate arrays, if a levels dimension exists Can be undone by postprocess.merge_levels</p> <p>Parameters:</p> <ul> <li> <code>dataset_with_levels</code>               (<code>Dataset</code>)           \u2013            <p>A xarray.dataset with dimensions [time,levels,width,height]</p> </li> </ul> <p>Returns:     dataset (xarray.Dataset): A xarray.dataset with dimensions [time,width,height]</p> Source code in <code>package/src/evametoc/data/preprocess.py</code> <pre><code>def split_levels(dataset_with_levels):\n    \"\"\"Splits levels-dimension in separate arrays, if a levels dimension exists\n    Can be undone by postprocess.merge_levels\n\n    Args:\n        dataset_with_levels (xarray.Dataset): A xarray.dataset with dimensions [time,levels,width,height]\n    Returns:\n        dataset (xarray.Dataset): A xarray.dataset with dimensions [time,width,height]\n    \"\"\"\n    data_xarrays = {}\n    for param_name in dataset_with_levels.keys():\n        data_xarray = dataset_with_levels[param_name]\n        if dataset_with_levels[param_name].ndim not in [3, 4]:\n            print(f'Skipping {param_name}, has less than 3 dimensions or more than 4')\n            continue\n        elif dataset_with_levels[param_name].ndim == 3:\n            # No levels, nothing to do\n            data_xarrays[param_name] = data_xarray\n        elif dataset_with_levels[param_name].ndim == 4:\n            # For each level create array and save to data_xarrays\n            level_dim_name = data_xarray.dims[1]\n            for levelidx in range(data_xarray.shape[1]):\n                flat_data_xarray = data_xarray[:, levelidx, ...]\n                flat_data_xarray = flat_data_xarray.squeeze(drop=True).reset_coords(drop=True)\n                level_values = dataset_with_levels.coords[level_dim_name][levelidx].values\n                flat_data_xarray.attrs[f'_z_{level_dim_name:s}'] = level_values\n                flat_param_name = f'{param_name:s}|z{levelidx:03d}' if data_xarray.shape[1] &gt; 1 else param_name\n                data_xarrays[flat_param_name] = flat_data_xarray\n    return xr.Dataset(data_xarrays)\n</code></pre>"},{"location":"reference/evametoc/video/ffmpeg.html","title":"ffmpeg","text":"<p>This module provides routines to convert write numpy-arrays to video files</p>"},{"location":"reference/evametoc/video/ffmpeg.html#evametoc.video.ffmpeg.decode","title":"<code>decode(video_file, metadata, metaspec, inkwargs=None, outkwargs=None, **kwargs)</code>","text":"<p>Reads a video file to a numpy-array of color values, using the ffmpeg-python module</p> <p>Parameters:</p> <ul> <li> <code>video_file</code>               (<code>str</code>)           \u2013            <p>path to the video file</p> </li> <li> <code>metadata</code>               (<code>dict</code>)           \u2013            <p>metadata for this video file</p> </li> <li> <code>metaspec</code>               (<code>dict</code>)           \u2013            <p>global metadata (such as shared_coords) present in the metadata-file</p> </li> <li> <code>inkwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>extra arguments to be passed for the input file in the ffmpeg module</p> </li> <li> <code>outkwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>extra arguments to be passed for the output stream in the ffmpeg module</p> </li> <li> <code>**kwargs</code>           \u2013            <p>same as inkwargs</p> </li> </ul> <p>Returns:     video_frames (numpy.ndarray dtype[uint8|uint16]) array with shape [colors,time,width,height] containing         the color values for each pixel     metadata (dict): metadata for this video file     metaspec (dict): global metadata (such as shared_coords) present in the metadata-file</p> Source code in <code>package/src/evametoc/video/ffmpeg.py</code> <pre><code>def decode(video_file, metadata, metaspec, inkwargs=None, outkwargs=None, **kwargs):\n    \"\"\"Reads a video file to a numpy-array of color values, using the ffmpeg-python module\n\n        Args:\n            video_file (str): path to the video file\n            metadata (dict): metadata for this video file\n            metaspec (dict): global metadata (such as _shared_coords_) present in the metadata-file\n            inkwargs (dict): extra arguments to be passed for the input file in the ffmpeg module\n            outkwargs (dict): extra arguments to be passed for the output stream in the ffmpeg module\n            **kwargs: same as inkwargs\n        Returns:\n            video_frames (numpy.ndarray dtype[uint8|uint16]) array with shape [colors,time,width,height] containing\n                the color values for each pixel\n            metadata (dict): metadata for this video file\n            metaspec (dict): global metadata (such as _shared_coords_) present in the metadata-file\n        \"\"\"\n    # Check function inputs\n    assert os.path.isfile(video_file), \"Video file does not exist\"\n    for ch in \"BGR\":\n        assert (\"coords\" in metadata[ch] or \"coord_names\" in metadata[ch]), \\\n            f\"The metadata for channel {ch:s} does not contain coordinates. Cannot convert to xarray.DataArray\"\n        assert \"dims\" in metadata[ch], \\\n            f\"The metadata for channel {ch:s} does not contain dimensions. Cannot convert to xarray.DataArray\"\n        for coord_name in metadata[ch].get(\"coord_names\", []):\n            assert coord_name in metaspec[\"_shared_coords_\"], \\\n                f\"The metadata for channel {ch:s} references \" \\\n                f\"coordinate '{coord_name:s}' that are not included in the file.\"\n\n    # Determine the dimension names\n    time_name = metadata[\"B\"]['dims'][0]\n    width_name = metadata[\"B\"]['dims'][1]\n    height_name = metadata[\"B\"]['dims'][2]\n\n    coords = metadata[\"B\"][\"coords\"] if \"coords\" in metadata[\"B\"] else metaspec[\"_shared_coords_\"]\n    # LEGACY In previous version coord was a list of values, now a dict with values and attributes.\n    timesteps = len(coords[time_name]['values'] if isinstance(coords[time_name], dict) else coords[time_name])\n    width = len(coords[width_name]['values'] if isinstance(coords[width_name], dict) else coords[width_name])\n    height = len(coords[height_name]['values'] if isinstance(coords[height_name], dict) else coords[height_name])\n    bitdepth = metadata[\"B\"].get('bitdepth', 8)\n\n    # Processing the input and output keywords\n    inkw = {}\n    inkw.update(inkwargs if inkwargs is not None else {})\n    inkw.update(kwargs)\n    outkw = {\n        'format': 'rawvideo',\n        'pix_fmt': 'bgr24' if bitdepth == 8 else 'gbrp16le',\n        'loglevel': 'error',\n        'pass': 1}\n    outkw.update(outkwargs if outkwargs is not None else {})\n\n    # Init the reader\n    ffmpeg_procedure = ffmpeg.input(video_file, **inkw).output('pipe:', **outkw).run_async(pipe_stdout=True)\n\n    # Read all bytes:\n    all_bytes = b''\n    while True:\n        read_bytes = ffmpeg_procedure.stdout.read(width * height)\n        if not read_bytes:\n            break\n        all_bytes += read_bytes\n    video_array = np.frombuffer(all_bytes, np.uint8 if bitdepth == 8 else np.uint16)\n\n    if 'gbrp' in outkw.get('pix_fmt', ''):\n        # gbrp16le has a slightly different order of pixels in the bitstream. Therefore, reorder\n        video_frames = np.transpose(video_array.reshape(timesteps, 3, width, height), (1, 0, 2, 3))\n        video_frames = np.concatenate([\n            video_frames[[1], :, :, :],  # Blue\n            video_frames[[0], :, :, :],  # Green\n            video_frames[[2], :, :, :],  # Red\n        ], axis=1)\n    else:\n        # bgr24\n        video_frames = np.transpose(video_array.reshape(timesteps, width, height, 3), (3, 0, 1, 2))\n\n    return video_frames, metadata, metaspec\n</code></pre>"},{"location":"reference/evametoc/video/ffmpeg.html#evametoc.video.ffmpeg.encode","title":"<code>encode(video_file, video_frames, metadata, *, file_size=None, codec='libx264', fps=24, inkwargs=None, outkwargs=None, **kwargs)</code>","text":"<p>Writes video frame arrays to a video file, using the ffmpeg-python module</p> <p>Parameters:</p> <ul> <li> <code>video_file</code>               (<code>str</code>)           \u2013            <p>Path to the file to be written</p> </li> <li> <code>video_frames</code>               (<code>numpy.ndarray dtype=[uint8|uint16]</code>)           \u2013            <p>A numpy array containing the encoded data as color-values, to be written to a video file</p> </li> <li> <code>metadata</code>               (<code>dict</code>)           \u2013            <p>metadata to be used for decoding the data, and storing coordinates</p> </li> <li> <code>file_size</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>Approximate number of bytes to be used to encode the video</p> </li> <li> <code>codec</code>               (<code>str</code>, default:                   <code>'libx264'</code> )           \u2013            <p>Codec to be used (see ffmpeg --codecs)</p> </li> <li> <code>fps</code>               (<code>float</code>, default:                   <code>24</code> )           \u2013            <p>frames per seconds</p> </li> <li> <code>inkwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>extra arguments to be passed for the input stream in the ffmpeg module</p> </li> <li> <code>outkwargs</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>extra arguments to be passed for the output file in the ffmpeg module</p> </li> <li> <code>**kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>same as outkwargs</p> </li> </ul> Source code in <code>package/src/evametoc/video/ffmpeg.py</code> <pre><code>def encode(video_file, video_frames, metadata, *,\n           file_size=None, codec='libx264', fps=24,\n           inkwargs=None, outkwargs=None, **kwargs):\n    \"\"\"Writes video frame arrays to a video file, using the ffmpeg-python module\n\n    Args:\n        video_file (str): Path to the file to be written\n        video_frames (numpy.ndarray dtype=[uint8|uint16]): A numpy array containing the encoded data as color-values,\n            to be written to a video file\n        metadata (dict): metadata to be used for decoding the data, and storing coordinates\n        file_size (int): Approximate number of bytes to be used to encode the video\n        codec (str): Codec to be used (see ffmpeg --codecs)\n        fps (float): frames per seconds\n        inkwargs (dict): extra arguments to be passed for the input stream in the ffmpeg module\n        outkwargs (dict): extra arguments to be passed for the output file in the ffmpeg module\n        **kwargs (dict): same as outkwargs\n    \"\"\"\n    # Checking the inputs\n    assert video_frames.ndim == 4, \"The data has to have 4 dimensions [color,time,lon,lat]\"\n    assert video_frames.shape[0] == 3, \"The data must have 3 colors\"\n    assert all(k in metadata.keys() for k in \"BGR\"), \"Metadata must have keys B, G or R\"\n\n    # Preparing the frames\n    video_frames = np.transpose(video_frames, (1, 2, 3, 0))  # color as last\n    timesteps, width, height, colors = video_frames.shape\n    bitdepth = 8  # metadata['B'].get('bitdepth', 8)\n\n    # Processing the input and output keywords\n    inkw = {\n        'f': 'rawvideo',\n        'pix_fmt': 'bgr24' if bitdepth == 8 else f'gbrp{bitdepth:02d}le',\n        'r': 24,\n        's': '{}x{}'.format(height, width)\n    }\n    inkw.update(inkwargs if inkwargs is not None else {})\n    outkw = {\n        'c:v': codec,\n        'pass': 1,\n        'r': fps,\n        'loglevel': 'error',\n    }\n    if file_size is not None:\n        outkw['b:v'] = (file_size * 8) / (1.073741824 * timesteps / fps)  # = Bitrate [bit/s]\n    outkw.update(outkwargs if outkwargs is not None else {})\n    outkw.update(kwargs)\n\n    # Init the video-writer\n    ffmpeg_procedure = (\n        ffmpeg\n        .input('pipe:', **inkw)\n        .output(video_file, **outkw)\n        .overwrite_output()\n        .run_async(pipe_stdin=True)\n    )\n\n    # Write the frames one by one\n    if 'gbrp' in inkw.get('pix_fmt', ''):\n        # Green, Blue, Red Planar (Plains before next color chanel)\n        video_frames = np.transpose(video_frames, (0, 3, 1, 2))\n        video_frames = np.concatenate([\n            video_frames[:, [1], :, :],  # Green\n            video_frames[:, [0], :, :],  # Blue\n            video_frames[:, [2], :, :],  # Red\n        ], axis=1)\n    for ti in range(timesteps):\n        ffmpeg_procedure.stdin.write(video_frames[ti, :, :, :].tobytes())\n\n    # Close the video-writer and write the metadata\n    ffmpeg_procedure.stdin.close()\n    ffmpeg_procedure.wait()\n    return video_file, metadata\n</code></pre>"},{"location":"reference/evametoc/video/frames.html","title":"frames","text":"<p>This module provides routines to convert xarrays and numpy-arrays to video-frames</p>"},{"location":"reference/evametoc/video/frames.html#evametoc.video.frames.from_numpy","title":"<code>from_numpy(data, metadata=None, bitdepth=8, nan_lossless=True)</code>","text":"<p>Writes three 3D-numpy arrays to a video file.</p> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>array</code>)           \u2013            <p>Numpy array to be encoded into a video file. Dimensions: channels==3,time,width,length</p> </li> <li> <code>metadata</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>data to be included into the metadata file. Keys must be B, G, or R for the blue (0), green (1) and red (r) channels</p> </li> <li> <code>bitdepth</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The number of bits to be used to encode each color. Must be 8, 10 or 16.</p> </li> <li> <code>nan_lossless</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Encode the NaN values lossless in the metadata, if False, nans will be encoded as 255, which may render unexpected results</p> </li> </ul> <p>Returns:     video_frames (numpy.ndarray dtype=[uint8|uint16]): A numpy array containing the encoded data as color-values,         to be written to a video file     metadata (dict): metadata to be used for decoding the data, and storing coordinates</p> Source code in <code>package/src/evametoc/video/frames.py</code> <pre><code>def from_numpy(data, metadata=None, bitdepth=8, nan_lossless=True):\n    \"\"\"Writes three 3D-numpy arrays to a video file.\n\n    Args:\n        data (np.array): Numpy array to be encoded into a video file. Dimensions: channels==3,time,width,length\n        metadata (dict): data to be included into the metadata file.\n            Keys must be B, G, or R for the blue (0), green (1) and red (r) channels\n        bitdepth (int): The number of bits to be used to encode each color. Must be 8, 10 or 16.\n        nan_lossless (bool): Encode the NaN values lossless in the metadata,\n            if False, nans will be encoded as 255, which may render unexpected results\n    Returns:\n        video_frames (numpy.ndarray dtype=[uint8|uint16]): A numpy array containing the encoded data as color-values,\n            to be written to a video file\n        metadata (dict): metadata to be used for decoding the data, and storing coordinates\n    \"\"\"\n\n    metadata = {\"B\": {}, \"G\": {}, \"R\": {}} if metadata is None else metadata\n\n    # Check function input\n    assert data.ndim == 4, \"The data has to have 4 dimensions [color,time,lon,lat]\"\n    assert data.shape[0] == 3, \"The data must have 3 colors\"\n    assert bitdepth in [8, 10, 16], \"Only a bitdepth of 8, 10 or 16 is supported\"\n    assert all(k in metadata.keys() for k in \"BGR\"), \"Metadata must have keys B, G or R\"\n\n    # Prepare the frames\n    frames = np.zeros(data.shape, dtype=np.uint8 if bitdepth == 8 else np.uint16)\n    max_color_value = 2 ** bitdepth - 1\n    for color in range(3):\n        channel_name = \"BGR\"[color]\n        channel_data = data[color, ...]\n        channel_min, channel_max = np.nanmin(channel_data), np.nanmax(channel_data)\n        nan_object = {}\n\n        if np.isnan(channel_data).any():\n            if nan_lossless:\n                nan_object = evametoc.data.nans.encode_nan(channel_data)\n                channel_data_f = evametoc.data.nans.fill_nans(channel_data)\n                channel_min, channel_max = np.nanmin(channel_data_f), np.nanmax(channel_data_f)\n                channel_scaled = (channel_data_f - channel_min) / (channel_max - channel_min) * max_color_value\n                channel = np.nan_to_num(channel_scaled, posinf=max_color_value, neginf=0)\n            else:\n                # Scale the data to 0..254 and nan to 255\n                channel_scaled = (channel_data - channel_min) / (channel_max - channel_min) * (max_color_value - 1)\n                channel = np.nan_to_num(channel_scaled, nan=max_color_value, posinf=max_color_value - 1, neginf=0)\n                nan_object = max_color_value\n        else:\n            # No NaNs so, scale the data to 0..255\n            channel_scaled = (channel_data - channel_min) / (channel_max - channel_min) * max_color_value\n            channel = np.nan_to_num(channel_scaled, posinf=max_color_value, neginf=0)\n            nan_object = -1\n        frames[color, :, :, :] = channel.astype('uint8' if bitdepth == 8 else 'uint16')\n        metadata[channel_name].update({\"min\": channel_min,\n                                       \"max\": channel_max,\n                                       \"nan\": nan_object,\n                                       \"bitdepth\": bitdepth})\n    return frames, metadata\n</code></pre>"},{"location":"reference/evametoc/video/frames.html#evametoc.video.frames.from_xarrays","title":"<code>from_xarrays(xarray_b, xarray_g, xarray_r, data_source='', shared_coords=False, **kwargs)</code>","text":"<p>Writes three xarray.DataArrays to numpy arrays for each frame, and the metadata.</p> <p>Parameters:</p> <ul> <li> <code>xarray_b</code>               (<code>DataArray</code>)           \u2013            <p>A data-array with dimensions [time,width,height], for the blue channel</p> </li> <li> <code>xarray_g</code>               (<code>DataArray</code>)           \u2013            <p>A data-array with dimensions [time,width,height], for the green channel</p> </li> <li> <code>xarray_r</code>               (<code>DataArray</code>)           \u2013            <p>A data-array with dimensions [time,width,height], for the red channel</p> </li> <li> <code>data_source</code>               (<code>str</code>, default:                   <code>''</code> )           \u2013            <p>source of the data, to be included in the metadata</p> </li> <li> <code>shared_coords</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>save unique coordinates only once, not with every array</p> </li> <li> <code>**kwargs</code>               (<code>dict</code>, default:                   <code>{}</code> )           \u2013            <p>keyword arguments to be passed to array_to_video_frames</p> </li> </ul> <p>Returns:     video_frames (numpy.ndarray dtype=[uint8|uint16]): A numpy array containing the encoded data as color-values,         to be written to a video file     metadata (dict): metadata to be used for decoding the data, and storing coordinates</p> Source code in <code>package/src/evametoc/video/frames.py</code> <pre><code>def from_xarrays(xarray_b, xarray_g, xarray_r, data_source=\"\", shared_coords=False, **kwargs):\n    \"\"\"Writes three xarray.DataArrays to numpy arrays for each frame, and the metadata.\n\n    Args:\n        xarray_b (xarray.DataArray): A data-array with dimensions [time,width,height], for the blue channel\n        xarray_g (xarray.DataArray): A data-array with dimensions [time,width,height], for the green channel\n        xarray_r (xarray.DataArray): A data-array with dimensions [time,width,height], for the red channel\n        data_source (str): source of the data, to be included in the metadata\n        shared_coords (bool): save unique coordinates only once, not with every array\n        **kwargs (dict): keyword arguments to be passed to array_to_video_frames\n    Returns:\n        video_frames (numpy.ndarray dtype=[uint8|uint16]): A numpy array containing the encoded data as color-values,\n            to be written to a video file\n        metadata (dict): metadata to be used for decoding the data, and storing coordinates\n    \"\"\"\n    # Check function parameters\n    assert len(xarray_b.shape) == 3, \"All arrays must have max 3 dimensions (B)\"\n    assert len(xarray_g.shape) == 3, \"All arrays must have max 3 dimensions (G)\"\n    assert len(xarray_r.shape) == 3, \"All arrays must have max 3 dimensions (R)\"\n    assert xarray_b.shape == xarray_r.shape, \"All arrays must have the same shape (B!=G)\"\n    assert xarray_b.shape == xarray_r.shape, \"All arrays must have the same shape (B!=R)\"\n\n    data = np.concatenate([\n        xarray_b.values[np.newaxis, :, :, :],\n        xarray_g.values[np.newaxis, :, :, :],\n        xarray_r.values[np.newaxis, :, :, :],\n    ])\n    if data_source:\n        xarray_b.attrs[\"source\"] = data_source\n        xarray_g.attrs[\"source\"] = data_source\n        xarray_r.attrs[\"source\"] = data_source\n    metadata = {\n        \"B\": {\"name\": xarray_b.name, \"dims\": xarray_b.dims, \"attrs\": xarray_b.attrs},\n        \"G\": {\"name\": xarray_g.name, \"dims\": xarray_g.dims, \"attrs\": xarray_g.attrs},\n        \"R\": {\"name\": xarray_r.name, \"dims\": xarray_r.dims, \"attrs\": xarray_r.attrs},\n    }\n\n    coords_b = {coord: {'values': np.atleast_1d(xarray_b.coords[coord].values), \"attrs\": xarray_b.coords[coord].attrs}\n                for coord in xarray_b.coords.keys()}\n    coords_g = {coord: {'values': np.atleast_1d(xarray_b.coords[coord].values), \"attrs\": xarray_b.coords[coord].attrs}\n                for coord in xarray_g.coords.keys()}\n    coords_r = {coord: {'values': np.atleast_1d(xarray_b.coords[coord].values), \"attrs\": xarray_b.coords[coord].attrs}\n                for coord in xarray_r.coords.keys()}\n    if shared_coords:\n        coords = coords_b\n        coords.update(coords_g)\n        coords.update(coords_r)\n        metadata[\"_shared_coords_\"] = coords\n        metadata[\"B\"][\"coord_names\"] = list(xarray_b.coords.keys())\n        metadata[\"G\"][\"coord_names\"] = list(xarray_g.coords.keys())\n        metadata[\"R\"][\"coord_names\"] = list(xarray_r.coords.keys())\n    else:\n        metadata[\"B\"][\"coords\"] = coords_b\n        metadata[\"G\"][\"coords\"] = coords_g\n        metadata[\"R\"][\"coords\"] = coords_r\n\n    return from_numpy(data, metadata=metadata, **kwargs)\n</code></pre>"},{"location":"reference/evametoc/video/frames.html#evametoc.video.frames.to_numpy","title":"<code>to_numpy(video_frames, metadata, metaspec=None)</code>","text":"<p>Reads a video to a numpy-array</p> <p>Parameters:</p> <ul> <li> <code>video_frames</code>               (<code>numpy.ndarray dtype[uint8|uint16]</code>)           \u2013            <p>pixel color values from a video file</p> </li> <li> <code>metadata</code>               (<code>dict</code>)           \u2013            <p>metadata of the video file</p> </li> <li> <code>metaspec</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>global metadata of the video files in this folder</p> </li> </ul> <p>Returns:     data (numpy.array): data in the video file, descaled using the metadata.         Dimensions [3 channels, time, width, height]     metadata (dict): metadata of the video file     metaspec (dict): global metadata of the video files in this folder</p> Source code in <code>package/src/evametoc/video/frames.py</code> <pre><code>def to_numpy(video_frames, metadata, metaspec=None):\n    \"\"\"Reads a video to a numpy-array\n\n    Args:\n        video_frames (numpy.ndarray dtype[uint8|uint16]): pixel color values from a video file\n        metadata (dict): metadata of the video file\n        metaspec (dict): global metadata of the video files in this folder\n    Returns:\n        data (numpy.array): data in the video file, descaled using the metadata.\n            Dimensions [3 channels, time, width, height]\n        metadata (dict): metadata of the video file\n        metaspec (dict): global metadata of the video files in this folder\n    \"\"\"\n    # Check function inputs\n    assert video_frames.ndim == 4, \"The data has to have 4 dimensions [color,time,lon,lat]\"\n    assert video_frames.shape[0] == 3, \"The data must have 3 colors\"\n    for ch in \"BGR\":\n        assert [\"min\" in metadata[ch] for ch in \"BGR\"], \\\n            f\"The metadata for channel {ch:s} does not a minimum value. Cannot convert to numpy.ndarray\"\n        assert [\"max\" in metadata[ch] for ch in \"BGR\"], \\\n            f\"The metadata for channel {ch:s} does not a maximum value. Cannot convert to numpy.ndarray\"\n\n    # Scale the data back to the original range\n    data_arrays = []\n    for color in range(3):\n        channel_name = \"BGR\"[color]\n        channel = video_frames[color, ...]\n\n        bitdepth = metadata[channel_name].get('bitdepth', 8)\n        max_color_value = 2 ** bitdepth - 1\n        channel_min, channel_max = float(metadata[channel_name]['min']), float(metadata[channel_name]['max'])\n        channel_data = channel\n\n        if isinstance(metadata[channel_name]['nan'], (int, float)):\n            if int(metadata[channel_name]['nan']) == -1:\n                # Scale the data back from color values [0..255] to data points\n                channel_data = (channel / max_color_value) * (channel_max - channel_min) + channel_min\n            else:\n                # Scale the data back from color values [0..254] to data points, 255 to nan\n                channel_data = (channel / (max_color_value - 1)) * (channel_max - channel_min) + channel_min\n                channel_data[channel == metadata[channel_name]['nan']] = np.nan\n        elif 'compression' in metadata[channel_name]['nan']:\n            channel_data = (channel / max_color_value) * (channel_max - channel_min) + channel_min\n            nan_array = evametoc.data.nans.decode_nan(metadata[channel_name]['nan'])\n            channel_data[nan_array] = np.nan\n        data_arrays.append(channel_data[np.newaxis, :, :, :])\n    data = np.concatenate(data_arrays)\n\n    return data, metadata, metaspec\n</code></pre>"},{"location":"reference/evametoc/video/frames.html#evametoc.video.frames.to_xarray","title":"<code>to_xarray(video_frames, metadata, metaspec=None)</code>","text":"<p>Reads a video to three xarray-DataArrays</p> <p>Parameters:</p> <ul> <li> <code>video_frames</code>               (<code>numpy.ndarray dtype[uint8|uint16]</code>)           \u2013            <p>pixel color values from a video file</p> </li> <li> <code>metadata</code>               (<code>dict</code>)           \u2013            <p>metadata of the video file</p> </li> <li> <code>metaspec</code>               (<code>dict</code>, default:                   <code>None</code> )           \u2013            <p>global metadata of the video files in this folder</p> </li> </ul> <p>Returns:     data (list[xarray.DataArray]): three DataArrays from the video file, descaled using the metadata,         combined with coordinates and attributes from the metadata</p> Source code in <code>package/src/evametoc/video/frames.py</code> <pre><code>def to_xarray(video_frames, metadata, metaspec=None):\n    \"\"\"Reads a video to three xarray-DataArrays\n\n    Args:\n        video_frames (numpy.ndarray dtype[uint8|uint16]): pixel color values from a video file\n        metadata (dict): metadata of the video file\n        metaspec (dict): global metadata of the video files in this folder\n    Returns:\n        data (list[xarray.DataArray]): three DataArrays from the video file, descaled using the metadata,\n            combined with coordinates and attributes from the metadata\n    \"\"\"\n    # Check function inputs\n    for ch in \"BGR\":\n        assert (\"coords\" in metadata[ch] or \"coord_names\" in metadata[ch]), \\\n            f\"The metadata for channel {ch:s} does not contain coordinates. Cannot convert to xarray.DataArray\"\n        assert \"dims\" in metadata[ch], \\\n            f\"The metadata for channel {ch:s} does not contain dimensions. Cannot convert to xarray.DataArray\"\n        for coord_name in metadata[ch].get(\"coord_names\", []):\n            assert coord_name in metaspec[\"_shared_coords_\"], \\\n                f\"The metadata for channel {ch:s} references coordinate '{coord_name:s}' that are not included.\"\n\n    # Frames to arrays\n    data, metadata, metaspec = to_numpy(video_frames, metadata, metaspec)\n\n    xarrays = []\n    for color in range(3):\n        channel_name = \"BGR\"[color]\n        channel_attrs = metadata[channel_name].get(\"attrs\", {})\n        channel_coords = {}\n        channel_coords_attrs = {}\n\n        # First add the dims, to make sure they are in the right order\n        for coord_name in metadata[channel_name][\"dims\"]:\n            if coord_name in metadata[channel_name].get(\"coords\", {}):\n                coord_object = metadata[channel_name][\"coords\"][coord_name]\n            else:\n                coord_object = metaspec[\"_shared_coords_\"][coord_name]\n            if isinstance(coord_object, dict):\n                coord_values = coord_object.get(\"values\", [])\n                channel_coords_attrs[coord_name] = coord_object.get(\"attrs\", {})\n            else:\n                coord_values = coord_object\n            channel_coords[coord_name] = coord_values\n\n        # Coordinates without dimension, or length=1, cannot be added to a xarray.DataArray object\n        for coord_name in metadata[channel_name].get(\"coords\", {}).keys():\n            if coord_name not in channel_coords:\n                coord_object = metadata[channel_name][\"coords\"][coord_name]\n                if isinstance(coord_object, dict):\n                    channel_attrs[coord_name] = coord_object.get(\"values\", [])\n                else:\n                    channel_attrs[coord_name] = coord_object\n        for coord_name in metadata[channel_name].get(\"coord_names\", []):\n            if coord_name not in channel_coords:\n                coord_object = metaspec[\"_shared_coords_\"][coord_name]\n                if isinstance(coord_object, dict):\n                    channel_attrs[coord_name] = coord_object.get(\"values\", [])\n                else:\n                    channel_attrs[coord_name] = coord_object\n\n        array_name = metadata[channel_name].get(\"name\", channel_name)\n        xarray_obj = xr.DataArray(\n            data[color, ...],\n            name=array_name,\n            attrs=channel_attrs,\n            coords=channel_coords)\n        for coord_name, coord_attrs in channel_coords_attrs.items():\n            xarray_obj.coords[coord_name].attrs = coord_attrs\n        xarrays.append(xarray_obj)\n    return xarrays\n</code></pre>"}]}